# Machine Learning

@(机器学习)

##  一、Linear Regression 

### 1.1 One variable case

引入问题：我们有一些房子的面积(size)和价格(price)的数据集，通过这样的数据集我们想得到能够预测任何一个size的房子的价格的假设。这里我们通过建立线性拟合模型来解答：

$$h_{\theta }= \theta_{0} + \theta_{1}x$$

同时我们先定义一些符号:

$m：数据实例的个数$
$(x_{i},y_{i})：	表示第 i 个数据集$
$(x,y)：实例集$

现在问题就转化为，寻找恰当的$\theta_{0}$和$\theta_{1}$使得我们的假设和实际数据集的$y$逼近，我们通过误差函数来反映这一逼近程度：

$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$

问题进一步转化为优化问题：寻找$\theta_{0}$和$\theta_{1}$来最小化$J(\theta_0,\theta_1)$;

#### 梯度下降(gradient descent)

想象自己在一个山坡上某一个点，自己要以最快的速度下山，该选择怎么样的路径。
![Alt text](./gradient descend.png)


具体的算法过程如下：

**重复下列过程直到误差函数收敛：**
$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$$
$$\alpha 是学习率，控制\theta_j的变化速度$$

>注意：每次遍历的时候，对于每个参数$\theta$都要同时更新，也就是说，新数据是批量产生的，而不是一个一个产生的。

![Alt text](./simultaneous update.png)

为何梯度下降方法能够起作用呢？我们先看一个简单的单变量问题：

$$\theta_1:=\theta_1-\alpha\frac{d}{d\theta_1}J(\theta_1)……式3$$

这里寻找$\theta_1$使得$J(\theta_1)$最小，下面是两个例子分析了梯度下降的运作过程：

![Alt text](./梯度下降理解.png)

从第一个图看出，选择$\theta_1$初始值在右边，一开始导数值为负，经过迭代，$\theta_1$可以朝着左边移动(也是向着最小值的方向移动的)，而且当有导数为0的时候，$\theta_1$就收敛了。同时当选择初值在右边也会朝着最优化的方向移动。

还有就是学习率$\alpha$的影响，如果学习率过大了之后，向左向右可能会越过最优解，不能收敛，所以要恰当地设置学习率。

从上图可以看出，无论是向左还是向右移动，导数项的绝对值是在减小的，也就是说每次迭代，$\theta_1$移动的步伐会放慢，这样会让其更好地收敛到最优解。

下面我们就可以把误差函数和梯度下降法结合起来分析了，得到一个真正的算法：

$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$
$$\theta_0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)$$
$$\theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)$$

其中两个导数项分别为：

$$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)$$
$$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)x_i$$
整理得算法为：

$$ \theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i) $$
$$\theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)x_i$$

从算法里面看到了求和项，需要遍历所有的实例来进行计算，这种梯度下降也叫“Batch” 梯度下降，计算量很大，后面我们可以继续优化。

### 1.2 multiple variables case
根据单变量的情形，很容易推出多变量的线性回归模型，首先建立有 $k$ 个输入变量的假设模型：

$$h_{\theta}(X^{(j)})=\theta_{0} + x^{(j)}_{1}*\theta_{1} + x^{(j)}_{2}*\theta_{2}+...+x^{(j)}_{k}*\theta_{k}$$

假设我们有输入矩阵$X$所有实例组成的矩阵，定义：

$X^{(j)}：代表第j个实例$
$X^{(j)}_{i}：代表第j个实例中第i个变量的值$
且我们让$X^{(j)}_{1}=1$，即所有实例的第一个变量值为1，则误差函数可以写成：

$\theta: (k+1)*1; X:n*(k+1)$

$$ J(\theta) = \frac{1}{2m}(X\theta - y)^T(X\theta - y)$$

可以求得，多变量线性回归的解析式：

$$\frac{\partial J(\theta)}{\partial \theta}= \frac{\partial}{\partial \theta}(X\theta - y)^T(X\theta - y) = 0$$

$$\frac{\partial}{\partial \theta}(\theta^TX^TX\theta-y^TX\theta-\theta^TX^Ty+y^Ty)=0$$

> 常见矩阵的微分：
>
> $\frac{\partial \theta^TX^TX\theta}{\partial \theta}=2X^TX\theta$
>
> $\frac{\partial y^TX\theta}{\partial \theta}=X^Ty$
>
> $\frac{\partial \theta^TX}{\partial \theta}=X$

带入可得：

$2X^TX\theta-X^Ty-X^Ty=0 \Rightarrow 2X^TX\theta=2X^Ty$

$\theta=(X^TX)^{-1}X^Ty$

## 二、Logistics Regression 

### 2.1 Introduction

有些时候我们想进行一些分类操作，比如垃圾邮件分类，判断图片中物品苹果还是香蕉等等，这个时候我们想要的输出是离散的，之前我们学习的线性回归的输出是连续的，我们也可以将连续的输出分区间来离散化，以此来模拟分类的输出。

现在有数据集$X,Y$，其中$y_i$表示每一个数据$x_i$的标签，并且$y_i \in \{0, 1\}$，那么如何建立模型能够解决该二分类的问题，理想的模型是能够告诉我们某个$x_i$对应得类别一定是0或者1，引入随机的概念后，$P(Y|X)$表示一个条件概率，也就是说模型可以给出某个类的概率是多少。那么我们下一步就是可以估计这个概率了，假设有下面的那个概率：
$$
P(Y=1|X=x) = p(x;\theta)
$$
那么用极大似然估计可以估计得到下面的式子：
$$
\prod_{i=1}^nP(Y=y_i|X=x_i)=\prod_{i=1}^np(x_i;\theta)^{y_i}(1-p(x_i;\theta))^{1-y_i}  
$$
接下来就是该如何建模$p(x_i, \theta)$了，$p$与$x$该建立怎样的关系，1）一种可以是简单的线性关系，不过这个不符合$y—$为$\{0,1\}$的取值范围。最简单的一种方式是使用logit函数:
$$
logit(p(x)) = log\frac{p(x)}{1-p(x)}=\beta_0 + x\cdot\beta
$$

$$
p(x,\beta) = \frac{1}{1+e^{-(\beta_0 + x\cdot \beta)}}
$$

本质上Logistic回归是去估计一个条件概率，然后再通过这个概率来判断标签是什么类别的，比如$P(Y=1|X=x_i)\geq 0.5$ 可以判别标签为1，小于0.5时可以判别标签为0。 

求解logistics参数$\beta_0、\beta$可以使用极大似然估计：
$$
Loss(\beta_0,\beta) = \prod_{i=1}^np(x_i;)^{y_i}(1-p(x_i))^{1-y_i}
$$
两边取对数：
$$
L(\beta_0, \beta) = \sum_{i=1}^n(y_ilog(p(x_i)) + (1-y_i)log(1-p(x_i)))
$$

$$
L(\beta_0,\beta) = \sum_{i=1}^n y_ilog(p(x_i)) + \sum_{i=1}^nlog(1-p(x_i)) - \sum_{i=1}^n y_ilog(1-p(x_i))
$$

$$
L(\beta_0, \beta) = \sum_{i=1}^ny_ilog\frac{p(x_i)}{1-p(x_i)} + \sum_{i=1}^nlog(1-p(x_i))
$$

$$
L(\beta_0,\beta) = \sum_{i=1}^ny_i(\beta_0+x\cdot\beta) + \sum_{i=1}^n-log(e^{\beta_0+x_i\cdot\beta} + 1)
$$

>$$1-p(x_i)=1 - \frac{1}{1+e^{-(\beta_0+x_i\cdot\beta)}} = \frac{e^{-(\beta_0+x_i\cdot\beta)}}{1+e^{-(\beta_0+x_i\cdot\beta)}} = \frac{\frac{1}{e^{\beta_0 + x_i\cdot\beta}}}{\frac{1+e^{\beta_0 + x_i\cdot\beta}}{e^{\beta_0 + x_i\cdot\beta}}} = \frac{1}{e^{\beta_0 + x_i\cdot\beta} + 1}$$
>
>$log(1-p(x_i)) = -log(e^{\beta_0+x_i\cdot\beta} + 1)$


$$
\frac{\partial L(\beta_0,\beta)}{\partial\beta_j} = - \sum_{i=1}^nx_{ij}(p(x_i;\beta_0,\beta)) + \sum_{i=1}^ny_ix_{ij}
$$


### 2.3 Advanced Optimization

在梯度下降的过程中，我们需要自己去设置学习率，并且时刻关注着学习率是否设置的合理，这个是很花费时间的工作，所以，这里有一些其它的优化算法，从而不需要设置学习率，同时收敛速度优于梯度下降方法，并且更适用于大规模的数据：

* Conjugate Gradient
* BFGS
* L-BFGS

这些算法不仅仅自动设置学习率，同时还有一些其它的有点，需要花时间来学习，也不建议自己去实现这些复杂的函数，而是应该去调用一些库文件来实现。

这里我们用Octave来实现一种自动选择学习率的算法，由于不需要设置学习率，我们只要在一个函数中计算误差函数$J(\theta)$的值，同时计算误差函数对于每个参数$\theta_{i}$的偏导数，并且返回，然后再设置一个初始的参数$\theta$，最后把函数和一些优化设置交给一个优化函数就可以了，在Octava中，这个优化函数是*fminunc*，通过help命令可以查看他的帮助:

```help
 -- fminunc (FCN, X0)
 -- fminunc (FCN, X0, OPTIONS)
 -- [X, FVAL, INFO, OUTPUT, GRAD, HESS] = fminunc (FCN, ...)
     Solve an unconstrained optimization problem defined by the function
     FCN. a pointer to function return the objective function value, optionally with gradient.
     X0 determines a starting guess
     OPTIONS. a optimset value,If "GradObj" is "on", it specifies that FCN, when called with two
     On return, X is the location of the minimum and FVAL contains the
   
```
针对只有一个变量的例子：先定义一个函数：
```
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```
设置优化选项和和初始参数，调用优化函数:
```
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```

## BootStrap

重复元素会导致结果的方差变大；一般bootstrap采样数量的样本数量（放回抽取），使用bootstrap来预估统计量，多次重复采取小样本计算统计量，然后用多个统计量的均值取估计目标统计量，不在bootstrap sample 里面的样本叫：out-of-bag 样本，bootstrap样本里面有许多重复的；估计了目标统计量后就可以求出统计量的confidence interval，用于评估模型的优劣；可以评估任意模型。

## Support Vector Machines

$E_{in}$ 和$E_{out}$ ，大量的数据好还是小样本数据好，经典统计学派和贝叶斯学派，连接主义学派神经网络。可以关注：概率图模型。

SVM基本原理：

1）用一个超平面来划分空间，从而实现二分类

2）用kernel来进行非线性变化以及对高维空间的识别

3）使用拉格朗子对偶性

4）泛化能力很强，目标是：**max margin**，结构化风险最小

如果不可分，可以把hard margin转为soft margin，允许小的数量的样本被划分错误，提高系统的容忍度。

1）$p$维空间的超平面是一个$p-1$维**仿射子空间**的一个平面（WIKI上面有例子）：

> 仿射空间：Alice 知道真正的原点，但是Bob认为另一个点$p$是原点，如果两个向量 $a$ 和 $b$相加，Bob的计算过程就是：$p+ (a-p) + (b-p)$

仿射空间相当于线性空间是做了平移的，简单来看就是忽略原点的存在的空间。在仿射空间和线性空间里面进行线性组合是等效的。

2）超平面的表示

$$\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0 $$

该超平面的法向量$\beta = (\beta_1, \beta_2, ..., \beta_p)$

3）划分数据的超平面可能有许多个，我们要选择一个**最优的**超平面。也就是选择：最大的margin，能使泛化能力最强。

构造问题：$$\max_{\beta} M$$

### Perceptron 感知器

创始人：Frank Rosenblatt；纪念他后创立了，Rosenblatt Award：

**PLA(Perceptron Learning Algorithm)**

$$f(x) = sign(wx + b)$$， w是法向量；wx +b =0 就是超平面；

误分类点到超平面的距离：$d = - \frac{1}{||w||}y_i(wx_i + b)$（任意一点$x$和超平面上一点$x^`$，连接的向量$(x,x^`)$在$w$上的投影）

损失函数为：

梯度为：

Batch gradient descent:

Stochastic Gradient Descent:

### Quadratic Programing（二次优化）



### 拉格朗日乘子法

> 推荐书籍：Convex Optimization

$$
x^* = argminf(x)
$$

$$
s.t \left\{\begin{matrix} c_i(x) \leqslant  0
 & i=1,2,3.... \\ h_j(x) = 0
 & j=1,2,3....
\end{matrix}\right.
$$

$$
L(x,\alpha,\beta)= f(x) + \sum_i\alpha_ic_i(x) + \sum_j\beta_jh_j(x)
$$

$$
min_x\theta_p(x) = min_xmax_{\alpha,\beta;\alpha_i>=0}L(x, \alpha,\beta)
$$

### KKT Condition

$$
\bigtriangledown_xL(x^*, \alpha^*, \beta^*) = 0
$$

$$
\alpha^*c_i(x^*) = 0
$$

$$
c_i(x^*) \leqslant  0
$$

$$
\alpha^* \geqslant  0
$$

$$
h_j(x^*) = 0
$$

### Linear SVM

$$
argmin_{w,b} \frac{1}{2}W^TW
$$

$$
s.t: y_i(x_i*w + b) \geqslant 1
$$

